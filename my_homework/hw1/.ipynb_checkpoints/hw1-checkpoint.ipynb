{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ROLLOUT_PATH='/home/puzi/RL/my_homework/hw1/rollouts'\n",
    "MAX_ITER=100000\n",
    "MAX_EPOCHS=100\n",
    "NUM_EPOCHS_VAL=1\n",
    "BATCH_SIZE=512\n",
    "HIDDEN_UNITS1=128\n",
    "HIDDEN_UNITS2=128\n",
    "BASE_LR=0.01\n",
    "NUM_LR_REDUCTIONS=3\n",
    "RENDER=True\n",
    "L2_REG_FACTOR=0.05\n",
    "VERBOSE=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ant-v1', 'Reacher-v1', 'Hopper-v1', 'Humanoid-v1', 'HalfCheetah-v1', 'Walker2d-v1']\n"
     ]
    }
   ],
   "source": [
    "envs=glob.glob(os.path.join(ROLLOUT_PATH,'*pkl'))\n",
    "envs=[os.path.basename(s)[:-12] for s in envs]\n",
    "print (envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ENV_NAME=envs[-1]\n",
    "fi=open(os.path.join(ROLLOUT_PATH,ENV_NAME+'_rollout.pkl'),'r')\n",
    "data=pickle.load(fi)\n",
    "fi.close()\n",
    "\n",
    "obs=data['observations']\n",
    "act=data['actions']\n",
    "rewards=data['returns']\n",
    "\n",
    "#random.shuffle(obs)\n",
    "#random.shuffle(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 17)\n",
      "(50000, 6)\n"
     ]
    }
   ],
   "source": [
    "act=act.reshape((act.shape[0],act.shape[-1]))\n",
    "print (obs.shape)\n",
    "print (act.shape)\n",
    "\n",
    "\n",
    "input_size=obs.shape[-1]\n",
    "output_size=act.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(obs, act, test_size=0.15)#, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42500, 17)\n",
      "(42500, 6)\n",
      "(7500, 17)\n",
      "(7500, 6)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_val.shape\n",
    "print y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ITER_PER_EPOCH=x_train.shape[0]/BATCH_SIZE+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mean=np.mean(x_train, axis=0)\n",
    "stdev=np.std(x_train, axis=0)\n",
    "\n",
    "x_train-=mean\n",
    "x_train/=stdev\n",
    "\n",
    "x_val-=mean\n",
    "x_val/=stdev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "source": [
    "x = tf.placeholder(tf.float32, [None, input_size])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, output_size])\n",
    "\n",
    "\n",
    "HIDDEN_UNITS1=128\n",
    "\n",
    "with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(tf.truncated_normal([input_size, HIDDEN_UNITS1],stddev=1.0 / math.sqrt(float(input_size))),name='weights')\n",
    "    #weights = tf.Variable(tf.truncated_normal([input_size, HIDDEN_UNITS1],stddev=1.0),name='weights')\n",
    "    biases = tf.Variable(tf.zeros([HIDDEN_UNITS1])+0.1,name='biases')\n",
    "    hidden1_reg=tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases)\n",
    "\n",
    "    hidden1 = tf.nn.relu(tf.matmul(x, weights) + biases)\n",
    "\n",
    "\n",
    "with tf.name_scope('regress'):\n",
    "    weights = tf.Variable(tf.truncated_normal([HIDDEN_UNITS1, output_size],stddev=1.0 / math.sqrt(float(output_size))),name='weights')\n",
    "    #weights = tf.Variable(tf.truncated_normal([HIDDEN_UNITS2, output_size],stddev=1.0),name='weights')\n",
    "    biases = tf.Variable(tf.zeros([output_size])+0.1,name='biases')\n",
    "    regress_reg=tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases)\n",
    "\n",
    "    output = tf.nn.tanh(tf.matmul(hidden1, weights) + biases)\n",
    "    pred_loss=tf.losses.mean_squared_error(y_,output)\n",
    "\n",
    "loss=pred_loss+L2_REG_FACTOR*(hidden1_reg+regress_reg)\n",
    "\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "#lr=tf.Variable(BASE_LR,name='lr',trainable=False)\n",
    "lr=tf.train.exponential_decay(BASE_LR,global_step, ITER_PER_EPOCH*(MAX_EPOCHS/NUM_LR_REDUCTIONS),0.01,staircase=True)\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "#optimizer=tf.train.MomentumOptimizer(learning_rate=lr,momentum=0.005,use_nesterov=True)\n",
    "\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "epoch_loss=[]\n",
    "epoch1_loss=[]\n",
    "val_dist=[]\n",
    "print \"Training Net\"\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    start=time.time()\n",
    "    if epoch>MAX_EPOCHS/NUM_LR_REDUCTIONS*2+10:\n",
    "        if float(val_dist[-10]-val_dist[-1])/val_dist[-1]<0.01:\n",
    "            print \"No improvemet in Val accuracy over last 10 epochs, terminating\"\n",
    "            break\n",
    "    #if epoch>0 and epoch%(MAX_EPOCHS/NUM_LR_REDUCTIONS)==0:\n",
    "        #lr=lr/float(100)\n",
    "    for iter in range(ITER_PER_EPOCH):\n",
    "        batch_x=x_train[:BATCH_SIZE,:]\n",
    "        batch_y=y_train[:BATCH_SIZE,:]\n",
    "        x_train=np.append(x_train[BATCH_SIZE:,:],x_train[:BATCH_SIZE,:],axis=0)\n",
    "        y_train=np.append(y_train[BATCH_SIZE:,:],y_train[:BATCH_SIZE,:],axis=0)\n",
    "        _,p_loss, loss_value = sess.run([train_op, pred_loss,loss],feed_dict={x:batch_x,y_:batch_y})\n",
    "        if epoch==0:\n",
    "            epoch1_loss.append(loss_value)\n",
    "            if VERBOSE:\n",
    "                print \"Epoch=0, Iteration=\",str(iter), \" loss=\",str(loss_value)\n",
    "    epoch_loss.append(loss_value)\n",
    "    print \"Finished running epoch=\",str(epoch), \"loss=\",str(loss_value),\"epoc duration=\",str(time.time()-start), \"seconds\"\n",
    "    if epoch%NUM_EPOCHS_VAL==0:\n",
    "        print \"Evaluating validation set\"\n",
    "        #pred=sess.run(output,feed_dict={x:x_val})\n",
    "        #pred=output\n",
    "        dist=tf.losses.mean_squared_error(y_val,output)\n",
    "        p,d=sess.run([output, dist],feed_dict={x:x_val})\n",
    "        val_dist.append(d)\n",
    "        print \"Total error=\", str(d)  \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, input_size])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, output_size])\n",
    "\n",
    "\n",
    "HIDDEN_UNITS1=128\n",
    "HIDDEN_UNITS2=64\n",
    "\n",
    "with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(tf.truncated_normal([input_size, HIDDEN_UNITS1],stddev=1.0 / math.sqrt(float(input_size))),name='weights')\n",
    "    #weights = tf.Variable(tf.truncated_normal([input_size, HIDDEN_UNITS1],stddev=1.0),name='weights')\n",
    "    biases = tf.Variable(tf.zeros([HIDDEN_UNITS1])+0.1,name='biases')\n",
    "    hidden1_reg=tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases)\n",
    "\n",
    "    hidden1 = tf.nn.relu(tf.matmul(x, weights) + biases)\n",
    "    #tf.summary.histogram('hidden1',hidden1)\n",
    "\n",
    "with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(tf.truncated_normal([HIDDEN_UNITS1, HIDDEN_UNITS2],stddev=1.0 / math.sqrt(float(HIDDEN_UNITS1))),name='weights')\n",
    "    #weights = tf.Variable(tf.truncated_normal([HIDDEN_UNITS1, HIDDEN_UNITS2],stddev=1.0),name='weights')\n",
    "    biases = tf.Variable(tf.zeros([HIDDEN_UNITS2])+0.1,name='biases')\n",
    "    hidden2_reg=tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases)\n",
    "\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    #tf.summary.histogram('hidden2',hidden2)\n",
    "\n",
    "with tf.name_scope('regress'):\n",
    "    weights = tf.Variable(tf.truncated_normal([HIDDEN_UNITS2, output_size],stddev=1.0 / math.sqrt(float(output_size))),name='weights')\n",
    "    #weights = tf.Variable(tf.truncated_normal([HIDDEN_UNITS2, output_size],stddev=1.0),name='weights')\n",
    "    biases = tf.Variable(tf.zeros([output_size])+0.1,name='biases')\n",
    "    regress_reg=tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases)\n",
    "\n",
    "    output = tf.nn.tanh(tf.matmul(hidden2, weights) + biases)\n",
    "    pred_loss=tf.losses.mean_squared_error(y_,output)\n",
    "    #tf.summary.histogram('output',output)\n",
    "    #tf.summary.scalar('pred_loss',pred_loss)\n",
    "\n",
    "loss=pred_loss+L2_REG_FACTOR*(hidden1_reg+hidden2_reg+regress_reg)\n",
    "#tf.summary.scalar('loss',loss)\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "lr=tf.train.exponential_decay(BASE_LR,global_step, ITER_PER_EPOCH*(MAX_EPOCHS/NUM_LR_REDUCTIONS),0.01,staircase=True)\n",
    "#tf.summary.scalar('lr',lr)\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "#optimizer=tf.train.MomentumOptimizer(learning_rate=lr,momentum=0.005,use_nesterov=True)\n",
    "\n",
    "\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "tf.summary.scalar('loss',loss)\n",
    "merged=tf.summary.merge_all()\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "writer=tf.summary.FileWriter('./reports',sess.graph)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "sess.run(init)\n",
    "epoch_loss=[]\n",
    "epoch1_loss=[]\n",
    "val_dist=[]\n",
    "print \"Training Net\"\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    start=time.time()\n",
    "    if epoch>MAX_EPOCHS/NUM_LR_REDUCTIONS*2+10:\n",
    "        if float(val_dist[-10]-val_dist[-1])/val_dist[-1]<0.01:\n",
    "            print \"No improvemet in Val accuracy over last 10 epochs, terminating\"\n",
    "            break\n",
    "    #if epoch>0 and epoch%(MAX_EPOCHS/NUM_LR_REDUCTIONS)==0:\n",
    "        #lr=lr/float(100)\n",
    "    for iter in range(ITER_PER_EPOCH):\n",
    "        batch_x=x_train[:BATCH_SIZE,:]\n",
    "        batch_y=y_train[:BATCH_SIZE,:]\n",
    "        x_train=np.append(x_train[BATCH_SIZE:,:],x_train[:BATCH_SIZE,:],axis=0)\n",
    "        y_train=np.append(y_train[BATCH_SIZE:,:],y_train[:BATCH_SIZE,:],axis=0)\n",
    "        _,p_loss, loss_value = sess.run([train_op, pred_loss,loss],feed_dict={x:batch_x,y_:batch_y})\n",
    "        #writer.add_summary(summary,iter)\n",
    "        if epoch==0:\n",
    "            epoch1_loss.append(loss_value)\n",
    "            if VERBOSE:\n",
    "                print \"Epoch=0, Iteration=\",str(iter), \" loss=\",str(loss_value)\n",
    "    epoch_loss.append(loss_value)\n",
    "    print \"Finished running epoch=\",str(epoch), \"loss=\",str(loss_value),\"epoc duration=\",str(time.time()-start), \"seconds\"\n",
    "    if epoch%NUM_EPOCHS_VAL==0:\n",
    "        print \"Evaluating validation set\"\n",
    "        #pred=sess.run(output,feed_dict={x:x_val})\n",
    "        #pred=output\n",
    "        dist=tf.losses.mean_squared_error(y_val,output)\n",
    "        p,d=sess.run([output, dist],feed_dict={x:x_val})\n",
    "        val_dist.append(d)\n",
    "        print \"Total error=\", str(d)  \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loss_chart=np.concatenate((epoch1_loss[0],val_dist))\n",
    "train_loss=plt.plot(epoch1_loss[0]+epoch_loss,'b-')\n",
    "plt.setp(train_loss, 'label','training loss')\n",
    "val_loss=plt.plot(epoch1_loss[0]+val_dist,'ro')\n",
    "plt.setp(val_loss, 'label','validation loss')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Training of '+ENV_NAME)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RENDER=False #Comment this line to see env rendering\n",
    "test_rewards=[]\n",
    "env=gym.make(ENV_NAME)\n",
    "max_steps=env.spec.timestep_limit\n",
    "for i in range(len(rewards)):\n",
    "    print \"Running rollout \", str(i)\n",
    "    obs = env.reset()\n",
    "    obs-=mean\n",
    "    obs/=stdev\n",
    "    reward=0\n",
    "    steps=0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action=sess.run(output,feed_dict={x:obs[None,:]})\n",
    "        #print action\n",
    "        obs,r,done,_=env.step(action)\n",
    "        obs-=mean\n",
    "        obs/=stdev\n",
    "        reward+=r\n",
    "        steps+=1\n",
    "        if steps % 100 == 0: \n",
    "            print(\"%i/%i\"%(steps, max_steps))\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        if steps>max_steps:\n",
    "            break\n",
    "    print str(steps),str(reward)\n",
    "    test_rewards.append(reward)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bbox=plt.boxplot([rewards,test_rewards])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tf_util\n",
    "import load_policy\n",
    "\n",
    "D_INIT_SIZE=40000\n",
    "NUM_DAgger_ITERS=10\n",
    "NUM_OBS_PER_ITER=10000\n",
    "\n",
    "D_x=x_train[:D_INIT_SIZE,:]\n",
    "D_y=y_train[:D_INIT_SIZE,:]\n",
    "\n",
    "policy_fn = load_policy.load_policy(os.path.join('./experts/',ENV_NAME+'.pkl'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)\n",
    "epoch_loss=[]\n",
    "val_dist=[]\n",
    "print \"Training Net\"\n",
    "for i in range(NUM_DAgger_ITERS):\n",
    "    print \"Running DAgger iteration #:\", str(i)\n",
    "    #Train net on D\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        start=time.time()\n",
    "        for iter in range(ITER_PER_EPOCH):\n",
    "            batch_x=D_x[:BATCH_SIZE,:]\n",
    "            batch_y=D_y[:BATCH_SIZE,:]\n",
    "            D_x=np.append(D_x[BATCH_SIZE:,:],D_x[:BATCH_SIZE,:],axis=0)\n",
    "            D_y=np.append(D_y[BATCH_SIZE:,:],D_y[:BATCH_SIZE,:],axis=0)\n",
    "            _, loss_value = sess.run([train_op, loss],feed_dict={x:batch_x,y_:batch_y})\n",
    "        epoch_loss.append(loss_value)\n",
    "        print \"Finished running epoch=\",str(epoch), \"loss=\",str(loss_value),\"epoc duration=\",str(time.time()-start), \"seconds\"\n",
    "        if epoch%NUM_EPOCHS_VAL==0:\n",
    "            print \"Evaluating validation set\"\n",
    "            #pred=sess.run(output,feed_dict={x:x_val})\n",
    "            #pred=output\n",
    "            dist=tf.losses.mean_squared_error(y_val,output)\n",
    "            p,d=sess.run([output, dist],feed_dict={x:x_val})\n",
    "            val_dist.append(d)\n",
    "            print \"Total error=\", str(d)\n",
    "    \n",
    "    #Generate new observations\n",
    "    new_obs=[]\n",
    "    while len(new_obs)<NUM_OBS_PER_ITER:\n",
    "        obs = env.reset()\n",
    "        obs-=mean\n",
    "        obs/=stdev\n",
    "        reward=0\n",
    "        steps=0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action=sess.run(output,feed_dict={x:obs[None,:]})\n",
    "            #print action\n",
    "            obs,r,done,_=env.step(action)\n",
    "            obs-=mean\n",
    "            obs/=stdev\n",
    "            reward+=r\n",
    "            steps+=1\n",
    "            if steps % 100 == 0: \n",
    "                print(\"%i/%i\"%(steps, max_steps))\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "            if steps>max_steps:\n",
    "                break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
